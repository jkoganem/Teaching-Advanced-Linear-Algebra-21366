{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Review of Basic Linear Algebra**\n",
    "\n",
    "---\n",
    "\n",
    "### **Introduction**\n",
    "This notebook goes over the fundamental concepts in Linear Algebra:\n",
    "* Vector spaces and subspaces\n",
    "* Independence, basis, dimension, rank of a matrix\n",
    "* Linear transformations and their matrix representations, change of basis\n",
    "* The four fundamental subspaces \n",
    "* The orthogonal decomposition theorem \n",
    "* Least squares solutions and projection matrices\n",
    "* Spectral theory, diagonalization \n",
    "* Fundamental matrix factorizations (LU, QR, Eigendecomposition, SVD)\n",
    "\n",
    "---\n",
    "\n",
    "### **Author**\n",
    "**Junichi Koganemaru**  \n",
    "\n",
    "---\n",
    "\n",
    "### **References**\n",
    "1. Gilbert Strang, Introduction to Linear Algebra.\n",
    "2. Stephen H. Friedberg, Arnold J. Insel, and Lawrence E. Spence, Linear Algebra.\n",
    "\n",
    "---\n",
    "\n",
    "### **Last Updated**\n",
    "**January 12, 2025**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Spaces\n",
    "\n",
    "> **Definition (Vector space)**\n",
    "A (real) vector space is a set $V$ on which two operations, $\\oplus_V$ (*vector addition*) and $\\otimes_V$ (*scalar multiplication*), are defined such that:\n",
    "> 1. For each pair of elements $x, y$ in the vector space $V$, there is a unique element $x \\oplus_V y$ in $V$.\n",
    "> 2. For each real number $c$ and each element $x$ in $V$, there is a unique element $c \\otimes_V x$ in $V$.\n",
    "> These operations satisfy the following conditions:\n",
    "> 1. **Commutativity:** For all $x, y \\in V$, $x \\oplus_V y = y \\oplus_V x$.\n",
    "> 2. **Associativity:** For all $x, y, z \\in V$, $(x \\oplus_V y) \\oplus_V z = x \\oplus_V (y \\oplus_V z)$.\n",
    "> 3. **Existence of zero vector $0_V$:** There exists an element $0_V \\in V$ such that $x \\oplus_V 0_V = x$ for all $x \\in V$.\n",
    "> 4. **Existence of additive inverses:** For each $x \\in V$, there exists $y \\in V$ such that $x \\oplus_V y = 0_V$.\n",
    "> 5. **Compatibility of scalar multiplication with field multiplication:** For all scalars $a, b$ and vector $x \\in V$, $(a b) \\otimes_V x = a \\otimes_V (b \\otimes_V x)$.\n",
    "> 6. **Distributivity of scalar multiplication with respect to vector addition:** For all scalars $a$ and vectors $x, y \\in V$, $a \\otimes_V (x \\oplus_V y) = (a \\otimes_V x) \\oplus_V (a \\otimes_V y)$.\n",
    "> 7. **Distributivity of scalar multiplication with respect to field addition:** For all scalars $a, b$ and vector $x \\in V$, $(a + b) \\otimes_V x = (a \\otimes_V x) \\oplus_V (b \\otimes_V x)$.\n",
    "> 8. **Identity element of scalar multiplication:** For all $x \\in V$, $1 \\otimes_V x = x$.\n",
    "\n",
    "Note: while technically a vector space should be denoted as $(V,\\oplus_V, \\otimes_V)$, when there is no risk of confusion we often simply denote it as $V$. \n",
    "\n",
    "## Important examples\n",
    "1. $\\mathbb{R}^n$, or in general $\\mathbb{F}^n$ where $\\mathbb{F}$ is a field.\n",
    "2. $\\mathcal{M}_{m \\times n}(\\mathbb{F})$, the vector space of $m \\times n$ matrices with values in $\\mathbb{F}$. \n",
    "3. $\\mathbb{P}_n[x]$, the vector space of polynomials of degree at most $n$ in the variable $x$.\n",
    "4. $C(\\mathbb{R} ; \\mathbb{R})$, the vector space of $\\mathbb{R}$-valued continuous functions over $\\mathbb{R}$. \n",
    "\n",
    "# Vector subspaces \n",
    "\n",
    "> **Definition** \n",
    "A subset $W$ of a vector space $V$ is said to be a *vector subspace* of $V$ if we can realize $W$ as a vector space (over the same field) with the operations of vector addition and scalar multiplication inherited from $V$. \n",
    "\n",
    "In other words, if $(V,+,\\cdot)$ is a vector space and $W$ is a subset of $V$, then $W$ is a subspace if $(W,+,\\cdot)$ is a vector space. \n",
    "\n",
    "> **Proposition (Criteria for subspace)**\n",
    "A subset $W \\subseteq V$ is a subspace of $V$ if:\n",
    "1. $W$ contains the zero vector $0_V$.\n",
    "2. $W$ is closed under vector addition.\n",
    "3. $W$ is closed under scalar multiplication.\n",
    "\n",
    "## Important examples\n",
    "1. The span of a set of vectors.\n",
    "2. The column space $\\text{Col}(A)$ of a matrix $A$. \n",
    "3. The nullspace $\\text{Null}(A)$ of a matrix $A$.\n",
    "4. The orthogonal complement $U^\\perp$ of a subspace $U$.\n",
    "\n",
    "\n",
    "## Independence, Basis, and Dimension\n",
    "\n",
    "> **Definition (Linear Independence)**\n",
    "A set of vectors $\\{v_1, v_2, \\dots, v_k\\}$ in a vector space $V$ is linearly independent if the only solution to the equation $$c_1 v_1 + c_2 v_2 + \\dots + c_k v_k = 0 $$ is $c_1 = c_2 = \\dots = c_k = 0$.\n",
    "\n",
    "Note: this definition can be extended to infinite subsets: an infinite subset of $V$ is said to be linearly independent if every non-empty finite subset of it is linearly independent.\n",
    "\n",
    "> **Definition (Basis)**  \n",
    "Let $V$ be a vector space and let $\\mathcal{B}$ be a collection of vectors in $V$. If the vectors in $\\mathcal{B}$ span $V$ (this means that $V = \\text{Span} \\,\\mathcal{B}$) and the vectors are linearly independent, then we call $\\mathcal{B}$ a *basis* for $V$.\n",
    "\n",
    "Therefore there are two criteria for a set $\\mathcal{B}$ to be a basis for a vector space $V$:\n",
    "\n",
    "1. We need $V = \\text{Span}\\,\\mathcal{B}$; in other words, every vector in $V$ must be a linear combination of the vectors in $\\mathcal{B}$.\n",
    "2. We also need $\\mathcal{B}$ to be linearly independent.\n",
    "\n",
    "> **Theorem**  \n",
    "Let $V$ be a vector space and let $J = \\{\\boldsymbol{w}_1, \\ldots , \\boldsymbol{w}_m\\}$ be such that $\\text{Span}\\,J = V$ for $m \\in \\mathbb{N}$. If $I = \\{\\boldsymbol{v}_1, \\ldots , \\boldsymbol{v}_n \\}$ is a linearly independent set in $V$ for $n \\in \\mathbb{N}$, then $n \\le m$.\n",
    "\n",
    "\n",
    "> **Proposition**  \n",
    "Let $V$ be a vector space and let $J = \\{\\boldsymbol{v}_1, \\ldots , \\boldsymbol{v}_m \\}$ be a basis for $V$, for $m \\in \\mathbb{N}$. Then any other basis for $V$ has the same number of elements in it.\n",
    "\n",
    "\n",
    "> **Definition (Dimension)**  \n",
    "Let $V$ be a vector space. If $\\mathcal{B} = \\{\\boldsymbol{v}_1, \\ldots , \\boldsymbol{v}_n \\}$ is a basis for $V$ and $n \\in \\mathbb{N}$, then the *dimension* of $V$ is $n$. Since $\\mathcal{B}$ is a set with finitely many elements, we refer to $V$ in this case as a *finite-dimensional* vector space. If $V$ does not admit a basis with finitely many elements, then we say that $V$ is an *infinite-dimensional* vector space.\n",
    "\n",
    "\n",
    "\n",
    "> **Definition (Row rank and column rank)**  \n",
    "The *row rank* of a matrix $A$ is the number of independent rows in $A$. The *column rank* of a matrix $A$ is the number of independent columns in $A$.\n",
    "\n",
    "\n",
    "> **Theorem (Rank-nullity theorem).**  \n",
    "Let $A$ be an $m \\times n$ matrix. Then $$\\dim \\text{Col}(A) + \\dim \\text{Null}(A) = n.$$\n",
    "\n",
    "> **Theorem (Row rank equals column rank).**  \n",
    "The row rank of a matrix is equal to its column rank.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear transformations\n",
    "\n",
    "> **Definition (Linear transformation)** \n",
    "A linear transformation $T: V \\to W$ between two vector spaces $V$ and $W$ is a function that maps a vector in $V$ to a vector in $W$, such that the two following properties hold. \n",
    "> * For any two vectors $\\boldsymbol{v}, \\boldsymbol{w}$ in $V$, we have $$T(\\boldsymbol{v} + \\boldsymbol{w}) = T \\boldsymbol{v} + T \\boldsymbol{v}.$$\n",
    "> * For any scalar $c$ and any vector $v$ in $V$, we have $$T(c \\boldsymbol{v}) = c T(\\boldsymbol{v}). $$\n",
    "\n",
    "> **Definition (Ordered bases)**\n",
    "Let $V$ be a vector space. An ***ordered basis*** is a basis for $V$ that is endowed with a specific order. \n",
    "\n",
    "> **Definition (Coordinate vector)** Let $V$ be an $n$-dimensional vector space, and let $\\mathcal{B} = \\{ \\boldsymbol{v}_1, \\ldots, \\boldsymbol{v}_n \\}$ be an ordered basis for $V$. Then, for any vector $\\boldsymbol{v}$ in $V$ we can find unique scalars $c_1,\\ldots, c_n$ such that $$ \\boldsymbol{v} = c_1 \\boldsymbol{v}_1 + \\ldots + c_n \\boldsymbol{v}_n. $$ We define the ***coordinate vector of x relative to $\\mathcal{B}$***, denoted by $[\\boldsymbol{v}]_\\mathcal{B}$, to be $$ [\\boldsymbol{v}]_\\mathcal{B} := \\begin{pmatrix} c_1 \\\\ c_2 \\\\ \\vdots \\\\ c_n \\end{pmatrix}. $$\n",
    "\n",
    "> **Definition (Matrix representation of a linear transformation)** Let $T : V \\to W$ be a linear transformation, and then $\\beta = \\{\\boldsymbol{v}_1, \\ldots , \\boldsymbol{v}_n \\}$, $\\gamma = \\{\\boldsymbol{w}_1, \\ldots , \\boldsymbol{w}_m\\}$ be ordered basis for $V,W$ respectively. Then the *matrix representation of $T$ with respect to the ordered basis $\\beta$ and $\\gamma$*, which we denote by $[T]_{\\beta}^\\gamma$, is the matrix $$ [T]_{\\beta}^\\gamma = \\begin{pmatrix} [T(\\boldsymbol{v}_1)]_{\\gamma} & \\ldots & [T (\\boldsymbol{v}_n) ]_{\\gamma} \\end{pmatrix}. $$\n",
    "\n",
    "The matrix representation is defined in a way so that \n",
    "$$\n",
    "\t T \\boldsymbol{x} = \\boldsymbol{y} \\Longleftrightarrow [T]_{\\beta}^\\gamma [\\boldsymbol{x} ]_{\\beta} = [\\boldsymbol{y} ]_\\gamma. \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surjective, injective, and bijective linear transformations\n",
    "\n",
    "> **Definition (Surjective, injective, and bijective linear transformations)**  \n",
    "Let $V,W$ be two vector spaces and let $T: V \\to W$ be a linear transformation.\n",
    "a) We say that $T$ is *surjective* if for every $\\boldsymbol{w} \\in W$, there exists at least one $\\boldsymbol{v} \\in V$ for which $T(\\boldsymbol{v}) = \\boldsymbol{w}$.  \n",
    "b) We say that $T$ is *injective* if $\\boldsymbol{x} \\neq \\boldsymbol{y} \\in V$, then $T(\\boldsymbol{x}) \\neq T(\\boldsymbol{y}) \\in W$. Equivalently, $T(\\boldsymbol{x}) = T(\\boldsymbol{y})$ in $W$ implies $\\boldsymbol{x} = \\boldsymbol{y}$.  \n",
    "c) We say that $T$ is *bijective* if $T$ is both injective and surjective.\n",
    "\n",
    "> **Proposition**  \n",
    "A linear transformation $T: V \\to W$ is injective if and only if the only element $\\boldsymbol{v} \\in V$ for which $T(\\boldsymbol{v}) = \\boldsymbol{0}_W$ is $\\boldsymbol{v} = \\boldsymbol{0}_V$.\n",
    "\n",
    "\n",
    "If $X,Y$ are finite dimensional, then we can also study $L$ through its matrix representation. In terms of the matrix representation of $L$:\n",
    "- $L$ is surjective if and only if its matrix representation has full row rank.\n",
    "- $L$ is injective if and only if its matrix representation has full column rank.\n",
    "- $L$ is bijective if and only if its matrix representation is invertible.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orthogonal subspace and Orthogonal complements, the four fundamental subspaces \n",
    "> **Definition (Orthogonal subspaces)**  \n",
    "Let $V,W$ be two subspaces of $\\mathbb{R}^n$. We say that $V,W$ are a pair of ***orthogonal subspaces*** (sometimes we say that $V,W$ are orthogonal to each other) if for all $\\boldsymbol{v}$ in $V$ and $\\boldsymbol{w}$ in $W$, $\\boldsymbol{v}^T \\boldsymbol{w} = 0$.\n",
    "\n",
    "> **Definition (Orthogonal complement)**  \n",
    "Let $V$ be a subspace of $\\mathbb{R}^n$. The ***orthogonal complement*** of $V, often denoted by $V^\\perp$, is the set containing every vector in $\\mathbb{R}^n$ perpendicular to every vector in $V$. In other words, $$ V^\\perp = \\{ \\boldsymbol{y} \\in \\mathbb{R}^n \\mid \\boldsymbol{y}^T \\boldsymbol{x} = 0 \\; \\text{for all} \\; \\boldsymbol{x} \\in V \\}. $$\n",
    "\n",
    "> **Proposition**  \n",
    "Let $V$ be a subspace of $\\mathbb{R}^n$. Then its orthogonal complement $V^\\perp$ is also a subspace.\n",
    "\n",
    "> **Proposition**  \n",
    "If $W \\subseteq V \\subseteq \\mathbb{R}^n$ are subspaces, then $V^\\perp \\subseteq W^\\perp$.\n",
    "\n",
    "> **Proposition**  \n",
    "Let $V$ be a subspace of $\\mathbb{R}^n$. Then $(V^\\perp)^\\perp = V$.\n",
    "\n",
    "> **Theorem ($\\text{Col}(A^T)$ is the orthogonal complement of $\\text{Null}(A)$)**  \n",
    "Let $A$ be an $m \\times n$ matrix. Then the nullspace of $A$ is the orthogonal complement of the column space of $A^T$ in $\\mathbb{R}^n$.\n",
    "\n",
    "> **Theorem ($\\text{Col}(A)$ is the orthogonal complement of $\\text{Null}(A^T)$)**  \n",
    "Let $A$ be an $m \\times n$ matrix. Then the nullspace of $A^T$ is the orthogonal complement of the column space of $A$ in $\\mathbb{R}^n$.\n",
    "\n",
    "> **Proposition (Orthogonal subspaces, complements, and their dimensions)**  \n",
    "Assume $V, W$ are subspaces of $\\mathbb{R}^n$ with $\\dim V = m, \\dim W = k$. Let $$ \\mathcal{B}_V = \\{ \\boldsymbol{v}_1 ,\\ldots, \\boldsymbol{v}_m \\} $$ be a basis for $V$, and $$ \\mathcal{B}_W= \\{ \\boldsymbol{w}_1, \\ldots, \\boldsymbol{w}_k \\} $$ be a basis for $W$. If $W = V^\\perp$, then \n",
    "> - $\\mathcal{B} = \\mathcal{B}_V \\cup \\mathcal{B}_W = \\{ \\boldsymbol{v}_1, \\ldots, \\boldsymbol{v}_m , \\boldsymbol{w}_1, \\ldots, \\boldsymbol{w}_k \\}$ is a basis for $\\mathbb{R}^n$,  \n",
    "> - $\\dim V + \\dim V^\\perp = n$.\n",
    "\n",
    "> **Definition**  \n",
    "Let $U,W$ be subspaces of $\\mathbb{R}^n$. We say that $V$ is the ***direct sum*** of $U$ and $W$ if $U \\cap W = \\{ \\boldsymbol{0} \\}$ and $V = U + W$. We often denote that $V is the direct sum of $U$ and $W$ by writing $$ V = U \\oplus W. $$\n",
    "\n",
    "> **Proposition (Direct sums and unique decompositions)**  \n",
    "Let $U,W$ be two subspaces of $\\mathbb{R}^n$ such that $U \\cap W = \\{ \\boldsymbol{0} \\}$. Let $V = U \\oplus W$ be their direct sum. Then the following hold.\n",
    "> - $V$ is also a vector subspace of $\\mathbb{R}^n$. \n",
    "> - $\\dim V = \\dim U + \\dim W$. \n",
    "> - If $\\mathcal{B}_U$ is a basis for $U$ and $\\mathcal{B}_W$ is a basis for $W$, then $\\mathcal{B} = \\mathcal{B}_U \\cup \\mathcal{B}_W$.\n",
    "> - Any vector $\\boldsymbol{v}$ in $V$ can be written ***uniquely*** as $$ \\boldsymbol{v} = \\boldsymbol{u} + \\boldsymbol{w}, $$ where $\\boldsymbol{u}$ is in $U$ and $\\boldsymbol{w}$ is in $W$.\n",
    "\n",
    "> **Corollary**  \n",
    "Let $V$ be a subspace of $\\mathbb{R}^n$, and let $V^\\perp$ be its orthogonal complement. Then $$ \\mathbb{R}^n = V \\oplus V^\\perp. $$\n",
    "\n",
    "> **Corollary**  \n",
    "Let $A$ be an $m \\times n$ matrix. Then any vector $\\boldsymbol{x}$ in $\\mathbb{R}^n$ can be written as $$ \\boldsymbol{x} = \\boldsymbol{x}_{\\text{Col}(A^T)} + \\boldsymbol{x}_{\\text{Null}(A)}, $$ where $ \\boldsymbol{x}_{\\text{Col}(A^T)}$ lives in the column space of $A^T$ and $\\boldsymbol{x}_{\\text{Null}(A)}$ lives in the nullspace of $A$, and $\\text{Col}(A^T)$, $\\text{Null}(A)$ are orthogonal complements. By Corollary, we can realize $\\mathbb{R}^n$ as a direct sum and write $$ \\mathbb{R}^n = \\text{Col}(A^T) \\oplus \\text{Null}(A). $$\n",
    "\n",
    "> **Corollary**  \n",
    "Let $A$ be an $m \\times n$ matrix. Then any vector $\\boldsymbol{y}$ in $\\mathbb{R}^m$ can be written as $$ \\boldsymbol{y} = \\boldsymbol{y}_{\\text{Col}(A)} + \\boldsymbol{y}_{\\text{Null}(A^T)}, $$ where $ \\boldsymbol{y}_{\\text{Col}(A)}$ lives in the column space of $A$ and $\\boldsymbol{y}_{\\text{Null}(A^T)}$ lives in the nullspace of $A^T$. By Corollary, we can realize $\\mathbb{R}^m$ as a direct sum and write $$ \\mathbb{R}^m = \\text{Col}(A) \\oplus \\text{Null}(A^T). $$\n",
    "\n",
    "> **Corollary**\n",
    "$\\text{Null}(A^T A) = \\text{Null}(A)$\n",
    "\n",
    "***Quick proof:*** $\\text{Null}(A) \\subseteq \\text{Null}(A^T A)$ is obvious. For the other direction, if $x \\in \\text{Null}(A^T A) \\Longleftrightarrow A^T A x = 0$ then $A x \\in \\text{Col}(A) = (\\text{Null}(A^T))^\\perp \\cap \\text{Null}(A^T)$, so it must be the case that $Ax = 0 \\Longleftrightarrow x \\in \\text{Null}(A)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orthogonal and semi-orthogonal matrices \n",
    "\n",
    "> **Definition (Orthogonal matrices)**  \n",
    "An $m \\times n$ matrix $Q$ with $m \\ge n$ is said to be a *semi-orthogonal matrix* if its columns are orthonormal vectors in $\\mathbb{R}^m$. If $m = n$, then we refer to $A$ as an *orthogonal matrix*.\n",
    "\n",
    "> **Proposition**  \n",
    "A matrix $Q \\in \\mathcal{M}_{m \\times n}(\\mathbb{R})$ with $m \\ge n$ is semi-orthogonal if and only if $Q^T Q = I_{n \\times n}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares solution and projection matrices \n",
    "\n",
    "> **Definition (Least squares solution)**  \n",
    "The least squares solution to the system $A \\boldsymbol{x} = \\boldsymbol{b}$ is the vector $\\boldsymbol{\\hat{x}}$ that minimizes the Euclidean norm of the residual vector $\\boldsymbol{r} = \\boldsymbol{b} - A \\boldsymbol{x}, \\boldsymbol{x} \\in \\mathbb{R}^n$. If $A$ has full column rank, then the least squares solution is given by $$ \\boldsymbol{\\hat{x}} = (A^T A)^{-1} A^T \\boldsymbol{b}. $$ An alternative characterization of the least squares solution is that it is the vector $\\boldsymbol{\\hat{x}}$ that specifies how one can combine columns of $A$ to get the projection of $\\boldsymbol{b}$ onto the column space of $A$, i.e. $\\text{proj}_{\\text{Col}(A)} \\boldsymbol{b} = A \\boldsymbol{\\hat{x}}$.\n",
    "\n",
    "> **Proposition**  \n",
    "Suppose $A$ is an $m \\times n$ matrix with $n$ independent columns. The projection matrix $P$ associated to the column space of $A$, defined via $$ P = A(A^T A)^{-1}A^T, $$ satisfies $P^2 = P$, $\\text{Col}(P) = \\text{Col}(A)$, $\\text{Null}(P) = (\\text{Col}(A))^\\perp$.\n",
    "\n",
    "> **Proposition (Projection as the best approximation)**  \n",
    "Let $V$ be a subspace of $\\mathbb{R}^m$ and suppose $\\boldsymbol{b}$ is not contained in $V$. Then the projection $\\boldsymbol{p}$ is the closest to the vector $\\boldsymbol{b}$ among all other vectors in $V$, in the sense that for all vectors $\\boldsymbol{w} \\neq \\boldsymbol{p}$ in $V$, we have $$ \\|\\boldsymbol{b} - \\boldsymbol{p}\\| < \\|\\boldsymbol{b} - \\boldsymbol{w}\\|. $$\n",
    "\n",
    "> **Corollary**  \n",
    "The orthogonal projection $\\boldsymbol{p}$ is the unique minimizer of the function $E: \\text{Col}(A) \\to \\mathbb{R}$ defined via $$ E(\\boldsymbol{w}) = \\|\\boldsymbol{b} - \\boldsymbol{w}\\|^2. $$ Equivalently, $\\boldsymbol{\\hat{x}}$ is the unique minimizer of the function $E: \\mathbb{R}^n \\to \\mathbb{R}$ defined via $$ E(\\boldsymbol{x}) = \\|\\boldsymbol{b} - A \\boldsymbol{x}\\|^2. $$ Another way of writing this is that $$ \\boldsymbol{\\hat{x}} = \\arg\\min_{\\boldsymbol{x} \\in \\mathbb{R}^n} \\|\\boldsymbol{b} - A \\boldsymbol{x}\\|^2. $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral theory, diagonalization\n",
    "### Spectral theory, diagonalization\n",
    "\n",
    "> **Definition (Eigenvalue and Eigenvector)**  \n",
    "Given a matrix $A$, a non-zero (but possibly complex) vector $\\boldsymbol{x}$ is said to be an *eigenvector* of $A$ if there exists a (possibly complex) scalar $\\lambda$ such that $$ A \\boldsymbol{x} = \\lambda \\boldsymbol{x}.$$ $\\lambda$ is what we refer to as the *eigenvalue* of $A$ associated to the eigenvector $\\boldsymbol{x}$.\n",
    "\n",
    "> **Proposition (Characteristic polynomial)**  \n",
    "Given an $n \\times n$ matrix $A$, $\\lambda$ is an eigenvalue of $A$ if and only if it is the root of the $n$-th degree polynomial $$ p_{A}(\\lambda) := \\det (A - \\lambda I), \\quad \\lambda \\in \\mathbb{C}. $$\n",
    "\n",
    "> **Proposition (Determinant and eigenvalues)**  \n",
    "Let $A$ be an $n \\times n$ matrix and let $\\lambda_1, \\ldots, \\lambda_n$ be its eigenvalues. Then $$ \\det(A) = \\lambda_1 \\ldots \\lambda_n. $$\n",
    "\n",
    "> **Definition (Trace)**  \n",
    "The sum of the diagonal entries of a square matrix $A$ is referred to as the *trace* of the matrix. We often denote it by $\\text{tr} A$.\n",
    "\n",
    "> **Proposition (Trace and eigenvalues)**  \n",
    "Let $A$ be an $n \\times n$ square matrix. Then the trace of $A$ is equal to the sum of its $n$ eigenvalues.\n",
    "\n",
    "> **Theorem**  \n",
    "Let $A \\in \\mathcal{M}_{n \\times n}(\\mathbb{R})$. Then $A$ has exactly $n$ eigenvalues up to multiplicity.\n",
    "\n",
    "> **Definition (Algebraic multiplicity)**  \n",
    "Let $A$ be an $n \\times n$ matrix and let $\\lambda$ be an eigenvalue of $A$. The ***algebraic multiplicity*** of $\\lambda$ is the multiplicity of $\\lambda$ as a root of the characteristic polynomial $p_A(\\lambda) = \\det(A - \\lambda I)$.\n",
    "\n",
    "> **Definition (Eigenspaces)**  \n",
    "Let $A$ be an $n \\times n$ matrix and let $\\lambda$ be an eigenvalue of $A$. The *$\\lambda$-eigenspace* of $A$ is the set of all vectors $\\boldsymbol{x}$ for which $$ A \\boldsymbol{x} = \\lambda \\boldsymbol{x}. $$ Equivalently, $$ \\lambda\\text{-eigenspace of} \\; A = \\{ \\text{eigenvectors of} \\; A \\; \\text{corresponding to the eigenvalue} \\; \\lambda\\} \\cup \\{ \\boldsymbol{0} \\}. $$\n",
    "\n",
    "> **Proposition**  \n",
    "Let $A$ be an $n \\times n$ matrix and let $\\lambda$ be a real eigenvalue of $A$. The $\\lambda$-eigenspace of $A$ is a vector subspace of $\\mathbb{R}^n$.\n",
    "\n",
    "> **Definition (Geometric multiplicity)**  \n",
    "Let $A$ be an $n \\times n$ matrix and let $\\lambda$ be an eigenvalue of $A$. The *geometric multiplicity* of $\\lambda$ is the dimension of the $\\lambda$-eigenspace of $A$.\n",
    "\n",
    "> **Proposition**  \n",
    "If $\\lambda$ is an eigenvalue of a matrix $A$, then its geometric multiplicity is greater or equal to 1 and is less than or equal to the algebraic multiplicity.\n",
    "\n",
    "> **Definition (Diagonalizable matrices)**  \n",
    "Let $A$ be an $n \\times n$ matrix. If there exists an invertible matrix $X$ and a diagonal matrix $\\Lambda$ such that $$ A = X \\Lambda X^{-1}, $$ then $A$ is said to be *diagonalizable*. If $X, \\Lambda$ are real matrices, then we say that $A$ is *diagonalizable over $\\mathbb{R}$*. If $X, \\Lambda$ are complex matrices, then we say that $A$ is *diagonalizable over $\\mathbb{C}$*.\n",
    "\n",
    "> **Proposition**  \n",
    "Let $A$ be a square $n \\times n$ matrix, and suppose that its eigenvalues $\\lambda_1 ,\\ldots, \\lambda_n$ are all distinct. Then we can find corresponding eigenvectors $\\boldsymbol{v}_1, \\ldots, \\boldsymbol{v}_n$ and the eigenvectors will be linearly independent.\n",
    "\n",
    "> **Corollary**  \n",
    "Let $A$ be an $n \\times n$ matrix with $n$ distinct eigenvalues. Then $A$ is diagonalizable.\n",
    "\n",
    "> **Definition (Symmetric matrices)**\n",
    "A square matrix $A \\in \\mathcal{M}_{m \\times n}(\\mathbb{R})$ is said to be *symmetric* if $A = A^T$.\n",
    "\n",
    "\n",
    "> **Theorem (Spectral theorem for real symmetric matrices)**  \n",
    "Every real symmetric matrix $S$ can be diagonalized and factorized as $$ S = Q \\Lambda Q^T, $$\n",
    "where $Q$ is an orthogonal matrix with eigenvectors of $S$ as its columns, and $\\Lambda$ is a diagonal matrix with the eigenvalues of $S$ on its diagonal.\n",
    "\n",
    "> **Corollary (Projection formula for symmetric matrices)**  \n",
    "Suppose $\\lambda_1, \\ldots, \\lambda_n$ are the eigenvalues of a real symmetric $n \\times n$ matrix $S$ and let $\\boldsymbol{q}_1, \\ldots, \\boldsymbol{q}_n$ be a set of corresponding orthonormal eigenvectors. Then $$ S = \\lambda_1 \\boldsymbol{q}_1 \\boldsymbol{q}_1^T + \\ldots +  \\lambda_n \\boldsymbol{q}_n \\boldsymbol{q}_n^T. $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positive definite matrices\n",
    "\n",
    "> **Definition (Positive definite matrices)**\n",
    "A symmetric matrix $A \\in \\mathcal{M}_{n \\times n}(\\mathbb{R})$ is said to be *positive definite* if for all non-zero vectors $\\boldsymbol{x}$, we have $$ \\boldsymbol{x}^T A \\boldsymbol{x} > 0. $$ If the inequality is not strict, then we say that $A$ is *positive semi-definite*.\n",
    "\n",
    "> **Proposition**\n",
    "A symmetric matrix $A$ is positive definite if and only if all of its eigenvalues are positive.\n",
    "\n",
    "> **Proposition**\n",
    "A symmetric matrix $A$ is positive semi-definite if and only if all of its eigenvalues are non-negative.\n",
    "\n",
    "**Remark:** One can similarly define negative definite and negative semi-definite matrices, and show that $A$ is negative definite if and only if $-A$ is positive definite, and $A$ is negative semi-definite if and only if $-A$ is positive semi-definite.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fundamental matrix factorizations\n",
    "\n",
    "- LU -decomposition: follows from row reduction.\n",
    "- QR-decomposition: follows from Gram-Schmidt.\n",
    "- Eigendecomposition: follows from finding eigenvalues and linearly independent eigenvectors.\n",
    "- Singular-value decomposition: follows from finding eigenvalues and orthonormal eigenvectors of $A^T A$ and $AA^T$."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
