{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Review of Basic Linear Algebra**\n",
    "\n",
    "---\n",
    "\n",
    "### **Introduction**\n",
    "This notebook goes over the fundamental concepts in Linear Algebra:\n",
    "* Vector spaces and subspaces\n",
    "* Independence, basis, dimension, rank of a matrix\n",
    "* Linear transformations and their matrix representations, change of basis\n",
    "* The four fundamental subspaces \n",
    "* The orthogonal decomposition theorem \n",
    "* Least squares soltuion and projection matrices \n",
    "* Fundamental matrix factorizations\n",
    "* Singular value decomposition \n",
    "\n",
    "---\n",
    "\n",
    "### **Author**\n",
    "**Junichi Koganemaru**  \n",
    "\n",
    "---\n",
    "\n",
    "### **References**\n",
    "1. Gilbert Strang, Introduction to Linear Algebra.\n",
    "2. Stephen H. Friedberg, Arnold J. Insel, and Lawrence E. Spence, Linear Algebra.\n",
    "\n",
    "---\n",
    "\n",
    "### **Last Updated**\n",
    "**January 11, 2025**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Spaces\n",
    "\n",
    "## Definition\n",
    "A (real) vector space is a set $V$ on which two operations, $\\oplus_V$ (*vector addition*) and $\\otimes_V$ (*scalar multiplication*), are defined such that:\n",
    "\n",
    "1. For each pair of elements $x, y$ in the vector space $V$, there is a unique element $x \\oplus_V y$ in $V$.\n",
    "2. For each real number $c$ and each element $x$ in $V$, there is a unique element $c \\otimes_V x$ in $V$.\n",
    "\n",
    "These operations satisfy the following conditions:\n",
    "\n",
    "1. **Commutativity:** For all $x, y \\in V$, $x \\oplus_V y = y \\oplus_V x$.\n",
    "2. **Associativity:** For all $x, y, z \\in V$, $(x \\oplus_V y) \\oplus_V z = x \\oplus_V (y \\oplus_V z)$.\n",
    "3. **Existence of zero vector $0_V$:** There exists an element $0_V \\in V$ such that $x \\oplus_V 0_V = x$ for all $x \\in V$.\n",
    "4. **Existence of additive inverses:** For each $x \\in V$, there exists $y \\in V$ such that $x \\oplus_V y = 0_V$.\n",
    "5. **Compatibility of scalar multiplication with field multiplication:** For all scalars $a, b$ and vector $x \\in V$, $(a b) \\otimes_V x = a \\otimes_V (b \\otimes_V x)$.\n",
    "6. **Distributivity of scalar multiplication with respect to vector addition:** For all scalars $a$ and vectors $x, y \\in V$, $a \\otimes_V (x \\oplus_V y) = (a \\otimes_V x) \\oplus_V (a \\otimes_V y)$.\n",
    "7. **Distributivity of scalar multiplication with respect to field addition:** For all scalars $a, b$ and vector $x \\in V$, $(a + b) \\otimes_V x = (a \\otimes_V x) \\oplus_V (b \\otimes_V x)$.\n",
    "8. **Identity element of scalar multiplication:** For all $x \\in V$, $1 \\otimes_V x = x$.\n",
    "\n",
    "Note: while technically a vector space should be denoted as $(V,\\oplus_V, \\otimes_V)$, when there is no risk of confusion we often simply denote it as $V$. \n",
    "\n",
    "## Important examples\n",
    "1. $\\mathbb{R}^n$, or in general $\\mathbb{F}^n$ where $\\mathbb{F}$ is a field.\n",
    "2. $\\mathcal{M}_{m \\times n}(\\mathbb{F})$, the vector space of $m \\times n$ matrices with values in $\\mathbb{F}$. \n",
    "3. $\\mathbb{P}_n[x]$, the vector space of polynomials of degree at most $n$ in the variable $x$.\n",
    "4. $C(\\mathbb{R} ; \\mathbb{R})$, the vector space of $\\mathbb{R}$-valued continuous functions over $\\mathbb{R}$. \n",
    "\n",
    "# Vector subspaces \n",
    "\n",
    "## Definition \n",
    "\n",
    "A subset $W$ of a vector space $V$ is said to be a *vector subspace* of $V$ if we can realize $W$ as a vector space (over the same field) with the operations of vector addition and scalar multiplication inherited from $V$. \n",
    "\n",
    "In other words, if $(V,+,\\cdot)$ is a vector space and $W$ is a subset of $V$, then $W$ is a subspace if $(W,+,\\cdot)$ is a vector space. \n",
    "\n",
    "**Proposition (Criteria for subspace)**\n",
    "A subset $W \\subseteq V$ is a subspace of $V$ if:\n",
    "1. $W$ contains the zero vector $0_V$.\n",
    "2. $W$ is closed under vector addition.\n",
    "3. $W$ is closed under scalar multiplication.\n",
    "\n",
    "## Important examples\n",
    "1. The span of a set of vectors.\n",
    "2. The column space $\\text{Col}(A)$ of a matrix $A$. \n",
    "3. The nullspace $\\text{Null}(A)$ of a matrix $A$.\n",
    "4. The orthogonal complement $U^\\perp$ of a subspace $U$.\n",
    "\n",
    "\n",
    "## Independence, Basis, and Dimension\n",
    "### Linear Independence\n",
    "A set of vectors $\\{v_1, v_2, \\dots, v_k\\}$ in a vector space $V$ is linearly independent if the only solution to the equation\n",
    "$$\n",
    "c_1 v_1 + c_2 v_2 + \\dots + c_k v_k = 0\n",
    "$$\n",
    "is $c_1 = c_2 = \\dots = c_k = 0$.\n",
    "\n",
    "Note: this definition can be extended to infinite subsets: an infinite subset of $V$ is said to be linearly independent if every non-empty finite subset of it is linearly independent.\n",
    "\n",
    "### Basis\n",
    "**Definition.**  \n",
    "Let $V$ be a vector space and let $\\mathcal{B}$ be a collection of vectors in $V$. If the vectors in $\\mathcal{B}$ span $V$ (this means that $V = \\text{Span} \\,\\mathcal{B}$) and the vectors are linearly independent, then we call $\\mathcal{B}$ a *basis* for $V$.\n",
    "\n",
    "Therefore there are two criteria for a set $\\mathcal{B}$ to be a basis for a vector space $V$:\n",
    "\n",
    "1. We need $V = \\text{Span}\\,\\mathcal{B}$; in other words, every vector in $V$ must be a linear combination of the vectors in $\\mathcal{B}$.\n",
    "2. We also need $\\mathcal{B}$ to be linearly independent.\n",
    "\n",
    "**Theorem.**  \n",
    "Let $V$ be a vector space and let $J = \\{\\boldsymbol{w}_1, \\ldots , \\boldsymbol{w}_m\\}$ be such that $\\text{Span}\\,J = V$ for $m \\in \\mathbb{N}$. If $I = \\{\\boldsymbol{v}_1, \\ldots , \\boldsymbol{v}_n \\}$ is a linearly independent set in $V$ for $n \\in \\mathbb{N}$, then $n \\le m$.\n",
    "\n",
    "\n",
    "**Proposition.**  \n",
    "Let $V$ be a vector space and let $J = \\{\\boldsymbol{v}_1, \\ldots , \\boldsymbol{v}_m \\}$ be a basis for $V$, for $m \\in \\mathbb{N}$. Then any other basis for $V$ has the same number of elements in it.\n",
    "\n",
    "### Dimension\n",
    "**Definition.**  \n",
    "Let $V$ be a vector space. If $\\mathcal{B} = \\{\\boldsymbol{v}_1, \\ldots , \\boldsymbol{v}_n \\}$ is a basis for $V$ and $n \\in \\mathbb{N}$, then the *dimension* of $V$ is $n$. Since $\\mathcal{B}$ is a set with finitely many elements, we refer to $V$ in this case as a *finite-dimensional* vector space. If $V$ does not admit a basis with finitely many elements, then we say that $V$ is an *infinite-dimensional* vector space.\n",
    "\n",
    "### Rank\n",
    "\n",
    "**Definition.**  \n",
    "The *row rank* of a matrix $A$ is the number of independent rows in $A$.\n",
    "\n",
    "**Definition.**  \n",
    "The *column rank* of a matrix $A$ is the number of independent columns in $A$.\n",
    "\n",
    "**Theorem (Rank-nullity theorem).**  \n",
    "Let $A$ be an $m \\times n$ matrix. Then\n",
    "$$\n",
    "\\dim \\text{Col}(A) + \\dim \\text{Null}(A) = n.\n",
    "$$\n",
    "\n",
    "**Theorem (Row rank equals column rank).**  \n",
    "The row rank of a matrix is equal to its column rank.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
